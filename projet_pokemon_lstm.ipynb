{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement de la DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille minimale des images  du dataset 272 , taille d'entrée des images de Resnet50 224\n",
      "\n",
      "taille des batchs d'images torch.Size([32, 3, 224, 224])\n",
      "\n",
      "taille des batchs légendes torch.Size([32, 18])\n",
      "\n",
      "taille des batchs types torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from PIL import Image\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "spacy_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=2):\n",
    "        self.itos = {\n",
    "            0: \"<PAD>\",\n",
    "            1: \"<SOS>\",\n",
    "            2: \"<EOS>\",\n",
    "            3: \"<UNK>\"\n",
    "        }\n",
    "        self.stoi = {\n",
    "            \"<PAD>\": 0,\n",
    "            \"<SOS>\": 1,\n",
    "            \"<EOS>\": 2,\n",
    "            \"<UNK>\": 3\n",
    "        }\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return [token.text.lower() for token in spacy_tokenizer.tokenizer(text)]\n",
    "\n",
    "    def make_vocabulary(self, sequences):\n",
    "        current_idx = 4\n",
    "        frequencies = {}\n",
    "\n",
    "        for sequence in sequences:\n",
    "            for word in self.tokenize(sequence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = current_idx\n",
    "                    self.itos[current_idx] = word\n",
    "                    current_idx += 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text]\n",
    "\n",
    "    def decode(self, sequence):\n",
    "        return [self.itos[token] if token in self.itos else \"<UNK>\" for token in sequence]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "\n",
    "class CustomPKMNDataset(Dataset):\n",
    "    \"\"\"\n",
    "\n",
    "    input : \n",
    "        img_dir   ->   str , emplacement image\n",
    "        data_file ->   str , emplacement fichier csv \n",
    "        transform ->       , transformation appliquée aux images (Resize)\n",
    "    \n",
    "    return :\n",
    "\n",
    "        Dataset   ->        , encodage images & légendes & types pokémons (tenseur torch)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir, data_file, transform=None):\n",
    "\n",
    "\n",
    "        self.img_dir = img_dir                 \n",
    "        self.data = pd.read_csv(data_file)\n",
    "        self.sequence_max = self.data['caption'].apply(lambda x: len(x.split())).max() \n",
    "        self.transform = transform    \n",
    "        self.vocab = Vocabulary()\n",
    "        self.vocab.make_vocabulary(self.data['caption'])      # encodage des légendes \n",
    "        self.unique_types = self.data['type'].unique()        # nombre de type \n",
    "        self.type_to_int = {type: idx for idx, type in enumerate(self.unique_types)} # encodage des types\n",
    "        self.int_to_type = {idx: type for type, idx in self.type_to_int.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ligne = self.data.iloc[idx]\n",
    "        image_name = ligne['image']  \n",
    "\n",
    "        img_path = os.path.join(self.img_dir, image_name)\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        caption = ligne['caption']\n",
    "        pokemon_type = ligne['type']\n",
    "\n",
    "        # Convertir la légende en indices\n",
    "        legende = [1]+ self.vocab.encode(caption)[1:] +[2]   # ne pas prendre en compte l'espace initial : ' '\n",
    " \n",
    "        add_padding =  dataset.sequence_max + 2 - len(legende)\n",
    "\n",
    "        legende += [0]*add_padding\n",
    "\n",
    "\n",
    "        # Encoder le type du Pokémon \n",
    "        type_encodage = self.type_to_int[pokemon_type]\n",
    "\n",
    "        return image, torch.tensor(legende,dtype=torch.long), torch.tensor(type_encodage,dtype=torch.long)\n",
    "    \n",
    "\n",
    "class PaddingCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=True,\n",
    "                               padding_value=self.pad_idx)\n",
    "        types = [item[2].unsqueeze(0) for item in batch]\n",
    "        types = torch.cat(types, dim=0)\n",
    "        return imgs, targets, types\n",
    "\n",
    "\n",
    "def make_loader(img_dir, data_file, transform, batch_size=32, num_workers=0, shuffle=True, pin_memory=True):\n",
    "    \"\"\"\n",
    "\n",
    "    return :  \n",
    "        (train_loader, test_loader) -> utilisés pour entrainer le RNN\n",
    "\n",
    "        train_all_laoder            -> utilisé pour entrainer le Resnet50\n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    dataset = CustomPKMNDataset(\n",
    "        img_dir, data_file, transform=transform)\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle,\n",
    "        pin_memory=pin_memory, collate_fn=PaddingCollate(pad_idx))\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle,\n",
    "        pin_memory=pin_memory, collate_fn=PaddingCollate(pad_idx))\n",
    "\n",
    "    train_all_loader = DataLoader(\n",
    "        dataset, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle,\n",
    "        pin_memory=pin_memory, collate_fn=PaddingCollate(pad_idx))\n",
    "\n",
    "\n",
    "    return train_all_loader , (train_loader,test_loader) , dataset\n",
    "\n",
    "\n",
    "class InfiniteDataLoader:\n",
    "    def __init__(self, dataset, batch_size=32, num_workers=0, shuffle=True, pin_memory=True):\n",
    "\n",
    "        train_sampler = torch.utils.data.RandomSampler(\n",
    "            dataset, replacement=True, num_samples=int(1e10))\n",
    "            \n",
    "        pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            dataset, sampler=train_sampler, batch_size=batch_size, num_workers=num_workers,\n",
    "        pin_memory=pin_memory, collate_fn=PaddingCollate(pad_idx))\n",
    "        self.data_iter = iter(self.train_loader)\n",
    "\n",
    "    def next(self):\n",
    "        try:\n",
    "            batch = next(self.data_iter)\n",
    "        except StopIteration:\n",
    "            self.data_iter = iter(self.train_loader)\n",
    "            batch = next(self.data_iter)\n",
    "        return batch\n",
    "\n",
    "\n",
    "img_dir = 'data/images/'\n",
    "data_file = 'data/data.csv'\n",
    "\n",
    "\n",
    "set_size = set()\n",
    "def print_image_dimensions(img_dir):\n",
    "    \"\"\"\n",
    "    \n",
    "    set_size  -> ensemble des tailles d'images possibles\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    for img_filename in os.listdir(img_dir):\n",
    "        img_path = os.path.join(img_dir, img_filename)\n",
    "        with Image.open(img_path) as img:\n",
    "             set_size.add(img.size[0])\n",
    "\n",
    "print_image_dimensions(img_dir)\n",
    "\n",
    "size_resnet = 224            # taille d'entrée des images de Resnet50\n",
    "min_size  = min(set_size)    # taille minimale des images  du dataset\n",
    "print(f\"taille minimale des images  du dataset {min_size} , taille d'entrée des images de Resnet50 {size_resnet}\")\n",
    "print()\n",
    "\n",
    "\n",
    "transform = transforms.Compose([      # transformation appliqué aux images -> shape(224,224) + tensor torch \n",
    "    transforms.Resize((size_resnet, size_resnet)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "train_all_loader , (train_loader,test_loader) , dataset= make_loader(img_dir , data_file ,transform ) # entrainement CNN\n",
    "\n",
    "batch_loader = InfiniteDataLoader(dataset)   # entrainement RNN\n",
    "\n",
    "images, legendes, types = next(iter(train_all_loader))\n",
    "\n",
    "print(f\"taille des batchs d'images {images.shape}\")\n",
    "print()\n",
    "print(f'taille des batchs légendes {legendes.shape}')\n",
    "print()\n",
    "print(f'taille des batchs types {types.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation du modèle Resnet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille de sortie torch.Size([32, 18])\n",
      "\n",
      "taille attendu torch.Size([32])\n",
      "\n",
      "cpu\n",
      "\n",
      "Modèle chargé avec succès.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet50(\n",
       "  (model): Sequential(\n",
       "    (0): ZeroPad2d((3, 3, 3, 3))\n",
       "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), bias=False)\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (5): ConvolutionalBlock(\n",
       "      (main_layers): ModuleList(\n",
       "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): IdentityBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): ConvolutionalBlock(\n",
       "      (main_layers): ModuleList(\n",
       "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): IdentityBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): IdentityBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (11): Flatten(start_dim=1, end_dim=-1)\n",
       "    (12): Linear(in_features=512, out_features=18, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class IdentityBlock(nn.Module):\n",
    "    def __init__(self, in_channels, filters, dropout_rate=0.3):\n",
    "        super(IdentityBlock, self).__init__()\n",
    "        f1, f2, f3 = filters\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, f1, kernel_size=1),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(f1, f2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(f2, f3, kernel_size=1),\n",
    "            nn.BatchNorm2d(f3)\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x += identity\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, filters, kernel_size, stride=2,  dropout_rate=0.3):\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "        f1, f2, f3 = filters\n",
    "\n",
    "        self.main_layers = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, f1, kernel_size=1, stride=stride),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(f1, f2, kernel_size=kernel_size, padding=1),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(f2, f3, kernel_size=1),\n",
    "            nn.BatchNorm2d(f3)\n",
    "        ])\n",
    "\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, f3, kernel_size=1, stride=stride),\n",
    "            nn.BatchNorm2d(f3)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "\n",
    "        for layer in self.main_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x += identity\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self,num_classes,layers=[2,3], dropout_rate=0.0):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # Initialiser les couches et les blocs\n",
    "        self.layers = [\n",
    "            nn.ZeroPad2d(3),\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        ]\n",
    "\n",
    "        # Ajouter les couches de ResNet50\n",
    "        self.layers += self._make_layer(64, [64, 64, 256], layers[0], stride=1)\n",
    "        self.layers += self._make_layer(256, [128, 128, 512], layers[1], stride=2)\n",
    "        #self.layers += self._make_layer(512, [256, 256, 1024], layers[2], stride=2)\n",
    "        #self.layers += self._make_layer(1024, [512, 512, 2048], layers[3], stride=2)\n",
    "\n",
    "        # Ajouter la couche fully connected\n",
    "        self.layers += [\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        ]\n",
    "\n",
    "        # Créer le modèle séquentiel\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "\n",
    "\n",
    "    def _make_layer(self, in_channels, filters, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(ConvolutionalBlock(in_channels, filters, kernel_size=3, stride=stride, dropout_rate=self.dropout_rate))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(IdentityBlock(filters[2], filters, self.dropout_rate))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def freeze_initial_layers(self,ind=2):\n",
    "        # methode pour eviter l'overfeating (freeze les poids de certaines couches)\n",
    "        # Geler les premières couches.\n",
    "        for layer in self.model[:ind]:  \n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x, return_intermediate = False):    \n",
    "        # permet de sortir les vecteurs avant leur passage dans la couche Fully connected \n",
    "        \n",
    "        \n",
    "        for layer in self.model[:-2]:\n",
    "            x = layer(x)\n",
    "        \n",
    "        if return_intermediate:\n",
    "            \n",
    "            return x\n",
    "        else:\n",
    "            \n",
    "            for layer in self.model[-2:]:\n",
    "                x = layer(x)\n",
    "            return x\n",
    "\n",
    "    \n",
    "num_classes = len(dataset.unique_types)\n",
    "\n",
    "cnn_model = ResNet50(num_classes)\n",
    "#cnn_model.freeze_initial_layers()   \n",
    "\n",
    "output = cnn_model(images)\n",
    "print(f'taille de sortie {output.shape}')\n",
    "print()\n",
    "print(f'taille attendu {types.shape}')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print()\n",
    "print(device)\n",
    "print()\n",
    "\n",
    "\n",
    "model_path = 'model_cnn.pth'\n",
    "find_model = True\n",
    "if os.path.isfile(model_path):\n",
    "    cnn_model.load_state_dict(torch.load(model_path,map_location=torch.device('cpu') ))\n",
    "    print(\"Modèle chargé avec succès.\")\n",
    "else:\n",
    "    print(\"Fichier modèle non trouvé.\")\n",
    "    find_model = False\n",
    "\n",
    "\n",
    "cnn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "lr = 1e-4\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "weight_decay = 0.00\n",
    "\n",
    "# optimiseur avec la régularisation L2 pour une meilleure généralisation \n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# en raison d'overfeating j'ai décidé de ne pas faire de test_laoder sur la partie CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not find_model :\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        avg_train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_idx , (images, _, types) in enumerate(train_all_loader): \n",
    "            cnn_model.train(True)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = cnn_model(images.to(device))\n",
    "            loss = loss_function(outputs, types.to(device).long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_train_loss += loss.item()\n",
    "            cnn_model.train(False)\n",
    "            with torch.no_grad():\n",
    "                _, predicted = torch.max(F.softmax(outputs).data, 1)\n",
    "                total += types.size(0)\n",
    "                correct += (predicted == types.to(device)).sum().item()\n",
    "\n",
    "        avg_train_loss /= batch_idx+1\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f} , Accuracy : {(100 * correct / total):.2f} % ')\n",
    "        print()\n",
    "\n",
    "        if epoch % 10 ==9 :\n",
    "            # save le model\n",
    "            torch.save(cnn_model.state_dict(), 'model_cnn.pth')\n",
    "\n",
    "        if abs(1 -correct / total )<1e-3:\n",
    "             break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille de sortie torch.Size([32, 512])\n",
      "\n",
      "taille attendu torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# test shape\n",
    "output = cnn_model(images.to(device),True).squeeze([2,3])\n",
    "print(f'taille de sortie {output.shape}')\n",
    "print()\n",
    "print(f'taille attendu {types.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation du Modèle RNN et Combined CNN-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methode avec LSTM  1 couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille du vocabulaire :  229 \n",
      "\n",
      "dimesion caché : 512 \n",
      "\n",
      "taille de l'embedding de position par image :512\n",
      "\n",
      "--------------------------------------------------------\n",
      "\n",
      "taille de sortie torch.Size([32, 17, 229])\n",
      "\n",
      "taille attendu torch.Size([32, 17])\n",
      "\n",
      "modif de la taille pour CrossEntropy :\n",
      "\n",
      "    |   output : torch.Size([544, 229]) \n",
      "    |   légende : torch.Size([544]) \n"
     ]
    }
   ],
   "source": [
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, hdim, input_cnn_size, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hdim)\n",
    "        self.lstm = nn.LSTM(input_size=hdim , hidden_size=hdim, num_layers=1, batch_first=True)\n",
    "        self.linear = nn.Linear(hdim , vocab_size)  \n",
    "        #self.linear_input = nn.Linear(input_cnn_size, hdim)\n",
    "        self.bn = nn.BatchNorm1d(hdim)  \n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, images_output, generation=False):\n",
    "        embedding_image = images_output#self.linear_input(images_output)\n",
    "        if not generation:\n",
    "            x = self.embedding(x[:, :-1])\n",
    "        else:\n",
    "            x = self.embedding(x)\n",
    "        \n",
    "        x = x + embedding_image.unsqueeze(1)\n",
    "\n",
    "\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        lstm_out, (h,_) = self.lstm(x)\n",
    "        \n",
    "\n",
    "        # Passage à travers la couche linéaire\n",
    "        out = self.linear(lstm_out)\n",
    "        if generation : \n",
    "            return self.linear(h)\n",
    "        return out  \n",
    "\n",
    "\n",
    "class Combined_cnn_LSTM(nn.Module):\n",
    "    def __init__(self,cnn_model , vocab_size , hdim  ,  input_ccn_size ):\n",
    "        super(Combined_cnn_LSTM, self).__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hdim = hdim\n",
    "        self.input_ccn_size = input_ccn_size\n",
    "        self.linear = nn.Linear(input_ccn_size,input_ccn_size)\n",
    "\n",
    "        for param in self.cnn_model.parameters():\n",
    "            param.requires_grad = False\n",
    "              \n",
    "        self.lstm_model = LSTM(vocab_size =self.vocab_size ,\n",
    "                                 hdim=self.hdim ,\n",
    "                                 input_cnn_size = self.input_ccn_size)\n",
    "\n",
    "    def forward(self, caption,images, generation =False ):\n",
    "        output_cnn = self.cnn_model(images,True).squeeze([2,3])\n",
    "        output_cnn = self.linear(output_cnn)\n",
    "        output = self.lstm_model(caption,output_cnn , generation)  \n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "images, captions, types = next(iter(train_loader))\n",
    "\n",
    "\n",
    "output = cnn_model(images.to(device),True).squeeze([2,3])\n",
    "_ , input_ccn_size =  output.shape         # taille de l'embedding de position de l'image (compréhension de l'image par le cnn)\n",
    "hdim = 512\n",
    "vocab_size = len(dataset.vocab)\n",
    "\n",
    "print(f\"taille du vocabulaire :  {vocab_size} \")\n",
    "print()\n",
    "print( f\"dimesion caché : {hdim} \")\n",
    "print()\n",
    "print( f\"taille de l'embedding de position par image :{input_ccn_size}\")\n",
    "print()\n",
    "print(\"--------------------------------------------------------\")\n",
    "print()\n",
    "model =Combined_cnn_LSTM(cnn_model = cnn_model,vocab_size =vocab_size ,hdim=hdim  ,input_ccn_size =  input_ccn_size)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "model.lstm_model.load_state_dict(torch.load('model_combined_LSTM_understand_2.pth',map_location=torch.device('cpu')))\n",
    "\n",
    "output = model(captions.to(device),images.to(device))\n",
    "\n",
    "print(f'taille de sortie {output.shape}')\n",
    "print()\n",
    "print(f'taille attendu {captions[:,1:].shape}')\n",
    "print()\n",
    "print(f'modif de la taille pour CrossEntropy :')\n",
    "print()\n",
    "print(f'    |   output : {output.view(-1, output.size(-1)).shape} ')\n",
    "print(f'    |   légende : {captions[:,1:].reshape(-1).shape} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(rnn, ixs,image,dataset):\n",
    "\n",
    "\n",
    "    for _ in range(dataset.sequence_max + 2):\n",
    "        ix_cond = ixs \n",
    "        logits = rnn(ix_cond,image , True ).squeeze(1)[-1:]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        _, max_indices = torch.max(probs, dim=-1)\n",
    "        ixs = torch.cat((ixs,max_indices.unsqueeze(-1) ), dim = -1)\n",
    "\n",
    "    return ixs\n",
    "\n",
    "@torch.no_grad()\n",
    "def print_samples(rnn, dataset, device):\n",
    "    \n",
    "    X_init = torch.zeros(1, 1, dtype=torch.long).to(device) +1\n",
    "\n",
    "\n",
    "    #  Sélectionner un entier au hasard entre 0 et len(dataset.data) - 1\n",
    "    random_index = random.randint(0, len(dataset.data) - 1)\n",
    "\n",
    "    image , legende = dataset.__getitem__(random_index)[0].to(device).unsqueeze(0) , dataset.__getitem__(random_index)[1]\n",
    "    X_samp = generate(rnn, X_init,image, dataset)\n",
    "\n",
    "    \n",
    "    X_samp = np.array(X_samp.cpu())\n",
    "    value  = dataset.vocab.decode(X_samp[0])\n",
    "    res = dataset.vocab.decode(np.array(legende.cpu())[1:-1])\n",
    "\n",
    "    def complet_sentence(liste):\n",
    "        sentence = []\n",
    "        for word in liste :\n",
    "            if (word == '<EOS>') | (word == '<PAD>'):\n",
    "                break \n",
    "            sentence.append(word)  \n",
    "        \n",
    "        return sentence\n",
    "\n",
    "    sentence = complet_sentence(value)\n",
    "    objetif =complet_sentence(res)\n",
    "\n",
    "\n",
    "    \n",
    "    # afficher l'image \n",
    "    ligne = dataset.data.iloc[random_index]\n",
    "    image_name = ligne['image']  \n",
    "    img_path = os.path.join(img_dir, image_name)\n",
    "    image = Image.open(img_path)\n",
    "    new_size = (100, 100) # par exemple, 100x100 pixels\n",
    "    resized_image = image.resize(new_size)\n",
    "\n",
    "\n",
    "    print('-----------------------------------------------')\n",
    "\n",
    "    print(f\"Image : {image_name} \")\n",
    "    display(resized_image)\n",
    "    print(\"Resultats :\")\n",
    "    print()\n",
    "    print(' '.join(sentence[1:]))\n",
    "    print()\n",
    "    print(\"Objetif : \")\n",
    "    print()\n",
    "    print(' '.join(objetif))\n",
    "    \n",
    "\n",
    "    print('-----------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Image : 174.png \n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=100x100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultats :\n",
      "\n",
      "a brown flying creature with elephant with elephant yellow butterfly on top of it\n",
      "\n",
      "Objetif : \n",
      "\n",
      "a very cute looking bird with big wings\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_samples(model , dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fp/hfmjvl7d45gft5_1gf2dzn640000gn/T/ipykernel_41563/2807083808.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  _, predicted = torch.max(F.softmax(logits.view(-1, logits.size(-1))).data, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 | loss 2.2693 | Accuracy : 58.64 %\n",
      "step 10 | loss 0.7992 | Accuracy : 79.61 %\n",
      "step 20 | loss 0.5173 | Accuracy : 82.96 %\n",
      "step 30 | loss 0.4598 | Accuracy : 85.53 %\n",
      "step 40 | loss 0.3367 | Accuracy : 87.41 %\n",
      "step 50 | loss 0.3688 | Accuracy : 88.66 %\n",
      "step 60 | loss 0.2707 | Accuracy : 89.74 %\n",
      "step 70 | loss 0.2300 | Accuracy : 90.28 %\n",
      "step 80 | loss 0.2196 | Accuracy : 91.49 %\n",
      "step 90 | loss 0.2204 | Accuracy : 91.93 %\n",
      "step 100 | loss 0.2331 | Accuracy : 92.10 %\n",
      "step 110 | loss 0.2004 | Accuracy : 92.70 %\n",
      "step 120 | loss 0.1896 | Accuracy : 93.46 %\n",
      "step 130 | loss 0.1982 | Accuracy : 93.33 %\n",
      "step 140 | loss 0.1831 | Accuracy : 94.14 %\n",
      "step 150 | loss 0.2012 | Accuracy : 93.95 %\n",
      "step 160 | loss 0.1412 | Accuracy : 95.02 %\n",
      "step 170 | loss 0.1640 | Accuracy : 95.06 %\n",
      "step 180 | loss 0.1255 | Accuracy : 95.22 %\n",
      "step 190 | loss 0.1250 | Accuracy : 95.44 %\n",
      "step 200 | loss 0.1000 | Accuracy : 95.70 %\n",
      "-----------------------------------------------\n",
      "Image : 662.png \n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=100x100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultats :\n",
      "\n",
      "a drawing of a pokemon pokemon with blue eyes\n",
      "\n",
      "Objetif : \n",
      "\n",
      "a pokemon ball with a bird on top of it\n",
      "-----------------------------------------------\n",
      "step 210 | loss 0.1549 | Accuracy : 95.28 %\n",
      "step 220 | loss 0.1255 | Accuracy : 95.77 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m images, legende \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), legende\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlegende\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(\n\u001b[1;32m     13\u001b[0m     logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m     14\u001b[0m     legende[:,\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/ApprentisageProf/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ApprentisageProf/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 53\u001b[0m, in \u001b[0;36mCombined_cnn_LSTM.forward\u001b[0;34m(self, caption, images, generation)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, caption,images, generation \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m ):\n\u001b[0;32m---> 53\u001b[0m     output_cnn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze([\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     54\u001b[0m     output_cnn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(output_cnn)\n\u001b[1;32m     55\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm_model(caption,output_cnn , generation)  \n",
      "File \u001b[0;32m~/ApprentisageProf/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ApprentisageProf/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 123\u001b[0m, in \u001b[0;36mResNet50.forward\u001b[0;34m(self, x, return_intermediate)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, return_intermediate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):    \n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# permet de sortir les vecteurs avant leur passage dans la couche Fully connected \u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m--> 123\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_intermediate:\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/ApprentisageProf/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ApprentisageProf/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 29\u001b[0m, in \u001b[0;36mIdentityBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n",
      "File \u001b[0;32m~/ApprentisageProf/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ApprentisageProf/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ApprentisageProf/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ApprentisageProf/venv/lib/python3.11/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step = 0 \n",
    "correct = 0\n",
    "total = 0\n",
    "while True:\n",
    "    images, legende, _ = batch_loader.next()\n",
    "    \n",
    "    images, legende = images.to(device), legende.to(device)\n",
    "\n",
    "    model.train(True)\n",
    "    logits = model(legende,images)    \n",
    "\n",
    "    loss = loss_function(\n",
    "        logits.view(-1, logits.size(-1)),\n",
    "        legende[:,1:].reshape(-1)\n",
    "    )\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    cnn_model.train(False)\n",
    "    with torch.no_grad():\n",
    "        _, predicted = torch.max(F.softmax(logits.view(-1, logits.size(-1))).data, 1)\n",
    "        total += legende[:,1:].reshape(-1).size(0)\n",
    "        correct += (predicted == legende[:,1:].reshape(-1)).sum().item()\n",
    "\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"step {step} | loss {loss.item():.4f} | Accuracy : {(100 * correct / total):.2f} %\")\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    if step > 0 and step % 200 == 0:\n",
    "        #print_samples(10, rnn, train_dataset, device)\n",
    "        print_samples(model , dataset, device)\n",
    "        torch.save(model.lstm_model.state_dict(), 'model_combined_LSTM_understand_2.pth')\n",
    "\n",
    "    if step > 0 and step % 500 == 0:\n",
    "        pass\n",
    "        #train_loss = eval(rnn, train_dataset, batch_size, 10, criterion)\n",
    "        #test_loss = eval(rnn, test_dataset, batch_size, 10, criterion)\n",
    "        #print(f\"step {step} train loss: {train_loss} test loss: {test_loss}\")\n",
    "\n",
    "    step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
